{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6b9e6de-a46a-4553-b764-47634385a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Activation, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda73da1-4543-46df-97cd-ba45a5cfd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Test loading the image\n",
    "img_path = r\"C:\\Users\\amalv\\OneDrive\\Desktop\\todo project\\CNN_Datasugmentation\\cats\\cat.jpeg\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e592da85-488e-4b45-a268-0da4f3bc5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(img_path)\n",
    "img_re = img.resize((200, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b2976-26d8-4ce8-aa9c-ffbcb763106c",
   "metadata": {},
   "source": [
    "# for 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a8e401-fab0-4f24-aa36-5c9dc3afaa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    # fill_mode=\"nearest\" build in and helps to add pixels to nearby place when image is moved\n",
    "    # fill_mode=\"reflect\" like a mirror\n",
    "    #fill_mode = \"constant\" is there whhich puts black image on top and bottom of the image \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca505b7-c139-4115-b6be-821beef29bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.img_to_array(img_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6ad312-e141-43e0-b0f0-3d2cc00fda3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a47554-e5fb-49be-ac71-0e06dc7b1012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9085736b-e67b-4ccb-a14f-aaba41dd24ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = img.reshape(1, 200, 200, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d297f34-9a19-4f18-91b2-58e0c1a24f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for output in datagen.flow(input_batch, batch_size=1, save_to_dir='aug'):\n",
    "\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb47cc4-39ca-4e06-8478-20433f6c8a70",
   "metadata": {},
   "source": [
    "# for multiple images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc9f5cdb-625b-4e93-a1da-69560d36dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = r'C:\\Users\\amalv\\OneDrive\\Desktop\\todo project\\CNN_Datasugmentation\\cat and dogs\\train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49523cd8-594a-46a3-9216-b589263974d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in categories:\n",
    "\n",
    "    folder_path = os.path.join(mydir,i)\n",
    "    \n",
    "    if i =='cats':\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "\n",
    "    for j in os.listdir(folder_path):\n",
    "\n",
    "        img_path = os.path.join(folder_path, j)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img,(150, 150))\n",
    "        data.append([img, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "351582bd-7f69-472a-b3ef-4564c19657a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46654670-34f5-4e04-aee7-da6dff9561a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in data:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "201072d7-b943-4409-a690-f78d24ad6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bd18a29-b65e-4c79-903c-86345224ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c1e8691-9a2b-47be-a5c0-f60b873fea82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 150, 150, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bb8a302-6a4b-40f4-be36-b5a2a2ea4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0149a8a-ec80-4d14-ac81-84fca99fe70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "320e08e5-69ec-4fe9-9e25-95ed280c5129",
   "metadata": {},
   "source": [
    "# create CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53891cc8-ddf0-4a44-9e6e-1da47df38189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amalv\\anaconda3\\jupyter\\envs\\myenv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Activation, Dropout\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3)))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f439792-aa52-4f44-9c4b-4374f728b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0be63f7b-3621-4b16-96b5-37e090101b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 250ms/step - accuracy: 0.8909 - loss: 0.2655 - val_accuracy: 0.6864 - val_loss: 0.8993\n",
      "Epoch 2/5\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 201ms/step - accuracy: 0.8976 - loss: 0.2306 - val_accuracy: 0.6982 - val_loss: 0.7207\n",
      "Epoch 3/5\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 203ms/step - accuracy: 0.9229 - loss: 0.2034 - val_accuracy: 0.7692 - val_loss: 0.6318\n",
      "Epoch 4/5\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 192ms/step - accuracy: 0.9444 - loss: 0.1706 - val_accuracy: 0.6982 - val_loss: 1.1267\n",
      "Epoch 5/5\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 191ms/step - accuracy: 0.9282 - loss: 0.1688 - val_accuracy: 0.7574 - val_loss: 0.8995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21181518610>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=5, validation_split=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e35c6-406a-4ff3-a4e0-5ccdcbf41a0c",
   "metadata": {},
   "source": [
    "# using image generator and trying agian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8aa18341-2504-4b20-884d-10b69a3adbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1682 images belonging to 2 classes.\n",
      "Found 210 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range= 0.2,\n",
    "    zoom_range= 0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'cat and dogs/train',\n",
    "    target_size =(150, 150),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "validate_generator = train_datagen.flow_from_directory(\n",
    "    'cat and dogs/test',\n",
    "    target_size =(150, 150),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d63d6046-5f7e-469e-9d02-c9d7f85f29b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amalv\\anaconda3\\jupyter\\envs\\myenv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3)))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6ccc90c-1656-463e-917e-4281d2d42339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amalv\\anaconda3\\jupyter\\envs\\myenv\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m106/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m5s\u001b[0m 285ms/step - accuracy: 0.5205 - loss: 0.7130 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amalv\\anaconda3\\jupyter\\envs\\myenv\\lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 273ms/step - accuracy: 0.5224 - loss: 0.7110 - val_accuracy: 0.5000 - val_loss: 0.7110\n",
      "Epoch 2/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 145ms/step - accuracy: 0.5672 - loss: 0.6891 - val_accuracy: 0.5714 - val_loss: 0.6656\n",
      "Epoch 3/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 147ms/step - accuracy: 0.6074 - loss: 0.6683 - val_accuracy: 0.6333 - val_loss: 0.6332\n",
      "Epoch 4/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 175ms/step - accuracy: 0.6565 - loss: 0.6274 - val_accuracy: 0.6095 - val_loss: 0.6688\n",
      "Epoch 5/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 142ms/step - accuracy: 0.6729 - loss: 0.6126 - val_accuracy: 0.6714 - val_loss: 0.6156\n",
      "Epoch 6/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 149ms/step - accuracy: 0.6868 - loss: 0.6081 - val_accuracy: 0.6667 - val_loss: 0.6028\n",
      "Epoch 7/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 148ms/step - accuracy: 0.7209 - loss: 0.5718 - val_accuracy: 0.5810 - val_loss: 0.6967\n",
      "Epoch 8/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 133ms/step - accuracy: 0.7077 - loss: 0.5632 - val_accuracy: 0.6667 - val_loss: 0.6248\n",
      "Epoch 9/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 129ms/step - accuracy: 0.7070 - loss: 0.5679 - val_accuracy: 0.6905 - val_loss: 0.5752\n",
      "Epoch 10/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 133ms/step - accuracy: 0.7114 - loss: 0.5595 - val_accuracy: 0.7190 - val_loss: 0.5367\n",
      "Epoch 11/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - accuracy: 0.7429 - loss: 0.5165 - val_accuracy: 0.6810 - val_loss: 0.5793\n",
      "Epoch 12/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 133ms/step - accuracy: 0.7536 - loss: 0.5223 - val_accuracy: 0.6810 - val_loss: 0.5427\n",
      "Epoch 13/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 136ms/step - accuracy: 0.7587 - loss: 0.5208 - val_accuracy: 0.6857 - val_loss: 0.5761\n",
      "Epoch 14/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 129ms/step - accuracy: 0.7575 - loss: 0.5162 - val_accuracy: 0.7286 - val_loss: 0.5520\n",
      "Epoch 15/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - accuracy: 0.7785 - loss: 0.4867 - val_accuracy: 0.7476 - val_loss: 0.5185\n",
      "Epoch 16/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 133ms/step - accuracy: 0.7651 - loss: 0.4901 - val_accuracy: 0.7190 - val_loss: 0.5746\n",
      "Epoch 17/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 133ms/step - accuracy: 0.7643 - loss: 0.4939 - val_accuracy: 0.7190 - val_loss: 0.5202\n",
      "Epoch 18/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 130ms/step - accuracy: 0.8039 - loss: 0.4552 - val_accuracy: 0.6667 - val_loss: 0.6540\n",
      "Epoch 19/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 131ms/step - accuracy: 0.7925 - loss: 0.4583 - val_accuracy: 0.7143 - val_loss: 0.5654\n",
      "Epoch 20/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 130ms/step - accuracy: 0.7833 - loss: 0.4667 - val_accuracy: 0.7333 - val_loss: 0.5188\n",
      "Epoch 21/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 130ms/step - accuracy: 0.7990 - loss: 0.4268 - val_accuracy: 0.7524 - val_loss: 0.5212\n",
      "Epoch 22/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 128ms/step - accuracy: 0.8087 - loss: 0.4313 - val_accuracy: 0.7810 - val_loss: 0.5484\n",
      "Epoch 23/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 131ms/step - accuracy: 0.8264 - loss: 0.3894 - val_accuracy: 0.6952 - val_loss: 0.7037\n",
      "Epoch 24/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - accuracy: 0.8070 - loss: 0.4120 - val_accuracy: 0.7286 - val_loss: 0.5301\n",
      "Epoch 25/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 134ms/step - accuracy: 0.8376 - loss: 0.3876 - val_accuracy: 0.7571 - val_loss: 0.5122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21186679ed0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch = 2000//batch_size,\n",
    "                   epochs=25,\n",
    "                   validation_data = validate_generator,\n",
    "                   validation_steps=800//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530cdab1-d235-4a5e-9bd4-4b60405fcc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
